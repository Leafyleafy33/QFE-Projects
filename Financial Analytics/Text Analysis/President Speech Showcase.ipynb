{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Question from Homework on Text Analysis:\n",
    "    - (a) Tokenize the texts into uni-grams (single words). Count the number of total tokens and the number of unique tokens in each text.\n",
    "    - (b) Now remove stopwords from the tokens and repeat the same count.\n",
    "    - (c) Count the number of times the token “america” appears in the text.\n",
    "    - (d) Count the number of times the token “union” appears in the text.\n",
    "    - (e) Count the number of times the token “freedom” appears in the text.\n",
    "    - (f) Count the number of times the token “constitution” appears in the text.\n",
    "    - (g) Now tokenize the texts into bi-grams (pairs of words). Count the number of times “united states” appears in the texts.\n",
    "    - (h) Based on the token counts, can we conclude anything about the theme of these presidential speeches?\n",
    "    - (i) Compute the FOG index for readability for each text. Comment on the result.\n",
    "    - (j) Construct the document feature matrix (DFM) of all the texts. Which are the top 5 features for each text?\n",
    "    - (k) Compute the cosine distance of texts in the DFM. Which two texts have the highest similarity? Which ones the lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(a). Token Counts\n",
      "=================\n",
      "+-----------------+----------------+-----------------+\n",
      "| Text            |  Total Tokens  |  Unique Tokens  |\n",
      "+=================+================+=================+\n",
      "| Biden_2021      |      2300      |       722       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Kennedy_1961    |      1332      |       530       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Lincoln_1861    |      3539      |      1007       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Trump_2017      |      1431      |       533       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Washington_1789 |      1394      |       592       |\n",
      "+-----------------+----------------+-----------------+\n",
      "=================\n",
      "\n",
      "\n",
      "(b). Token Counts with no Stopwords\n",
      "===================================\n",
      "+-----------------+----------------+-----------------+\n",
      "| Text            |  Total Tokens  |  Unique Tokens  |\n",
      "+=================+================+=================+\n",
      "| Biden_2021      |      972       |       577       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Kennedy_1961    |      612       |       412       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Lincoln_1861    |      1440      |       838       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Trump_2017      |      624       |       419       |\n",
      "+-----------------+----------------+-----------------+\n",
      "| Washington_1789 |      578       |       469       |\n",
      "+-----------------+----------------+-----------------+\n",
      "===================================\n",
      "\n",
      "\n",
      "(c)~(f). Specific Token Counts\n",
      "==============================\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "| Text            |  america  |  union  |  freedom  |  constitution  |\n",
      "+=================+===========+=========+===========+================+\n",
      "| Biden_2021      |    20     |    2    |     0     |       3        |\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "| Kennedy_1961    |     2     |    0    |     4     |       0        |\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "| Lincoln_1861    |     0     |   20    |     0     |       24       |\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "| Trump_2017      |    19     |    0    |     0     |       0        |\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "| Washington_1789 |     0     |    2    |     0     |       1        |\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "| Total           |    41     |   24    |     4     |       28       |\n",
      "+-----------------+-----------+---------+-----------+----------------+\n",
      "==============================\n",
      "\n",
      "\n",
      "(g). Bi-gram Counts for 'united states'\n",
      "=======================================\n",
      "+-----------------+-----------------+\n",
      "| Text            |  united states  |\n",
      "+=================+=================+\n",
      "| Biden_2021      |        1        |\n",
      "+-----------------+-----------------+\n",
      "| Kennedy_1961    |        0        |\n",
      "+-----------------+-----------------+\n",
      "| Lincoln_1861    |        5        |\n",
      "+-----------------+-----------------+\n",
      "| Trump_2017      |        2        |\n",
      "+-----------------+-----------------+\n",
      "| Washington_1789 |        2        |\n",
      "+-----------------+-----------------+\n",
      "=======================================\n",
      "\n",
      "\n",
      "(h). Themes of Each Text\n",
      "==================================================\n",
      "\n",
      "Biden & Trump:\n",
      "- Tokens of Interest: 'America'\n",
      "- Inference: Both use 'America' frequently, suggesting themes related to U.S.\n",
      "  internal politics and the nation's future outlook. Their focus seems centered on\n",
      "  evolving visions and potential trajectories for the United States.\n",
      "==================================================\n",
      "\n",
      "Lincoln:\n",
      "- Tokens of Interest: 'Union' and 'Constitution'\n",
      "- Inference: Lincoln's emphasis on 'Union' and 'Constitution' implies themes\n",
      "  around the American Civil War. The national mood was heavily influenced\n",
      "  by constitutional and legal matters, with tensions from potential southern\n",
      "  states' secession and the overarching theme of national unity.\n",
      "==================================================\n",
      "\n",
      "JFK's Inaugural Address:\n",
      "- Tokens of Interest: 'freedom'\n",
      "- Inference: JFK's emphasis on 'freedom' suggests a theme within the Cold War era,\n",
      "  portraying it as a dichotomy between freedom and communism,\n",
      "  emphasizing the pivotal role of freedom globally.\n",
      "==================================================\n",
      "\n",
      "Washington:\n",
      "- Tokens of Interest: 'union' and 'constitution'\n",
      "- Inference: Washington's focus on 'union' and 'constitution' hints at themes\n",
      "  of nation-building. Given the country's infancy and the ongoing constitution\n",
      "  ratification, there was an emphasis on fostering unity and establishing\n",
      "  a robust constitutional groundwork.\n",
      "==================================================\n",
      "\n",
      "\n",
      "(i). FOG Index for Each Text\n",
      "============================\n",
      "+-----------------+-------------+\n",
      "| Text            |  FOG Index  |\n",
      "+=================+=============+\n",
      "| Biden_2021      |    8.95     |\n",
      "+-----------------+-------------+\n",
      "| Kennedy_1961    |    15.56    |\n",
      "+-----------------+-------------+\n",
      "| Lincoln_1861    |    18.21    |\n",
      "+-----------------+-------------+\n",
      "| Trump_2017      |    12.32    |\n",
      "+-----------------+-------------+\n",
      "| Washington_1789 |    34.1     |\n",
      "+-----------------+-------------+\n",
      "============================\n",
      "\n",
      "\n",
      "(i). FOG Index for Each Text (spaCy)\n",
      "====================================\n",
      "+-----------------+-------------+\n",
      "| Text            |  FOG Index  |\n",
      "+=================+=============+\n",
      "| Biden_2021      |    9.39     |\n",
      "+-----------------+-------------+\n",
      "| Kennedy_1961    |    16.07    |\n",
      "+-----------------+-------------+\n",
      "| Lincoln_1861    |    17.92    |\n",
      "+-----------------+-------------+\n",
      "| Trump_2017      |    12.56    |\n",
      "+-----------------+-------------+\n",
      "| Washington_1789 |    34.2     |\n",
      "+-----------------+-------------+\n",
      "====================================\n",
      "\n",
      "\n",
      "(j). Top 5 Features for Each Text\n",
      "=================================\n",
      "+-----------------+--------------------------------------------------------------------------+\n",
      "| Text            | Top 5 Features                                                           |\n",
      "+=================+==========================================================================+\n",
      "| Biden_2021      | america (20), nation (14), democracy (11), american (9), americans (9)   |\n",
      "+-----------------+--------------------------------------------------------------------------+\n",
      "| Kennedy_1961    | let (16), sides (8), world (8), new (7), pledge (7)                      |\n",
      "+-----------------+--------------------------------------------------------------------------+\n",
      "| Lincoln_1861    | constitution (24), people (20), union (20), states (19), government (18) |\n",
      "+-----------------+--------------------------------------------------------------------------+\n",
      "| Trump_2017      | america (19), american (11), people (10), country (9), nation (8)        |\n",
      "+-----------------+--------------------------------------------------------------------------+\n",
      "| Washington_1789 | government (8), public (6), citizens (5), country (5), present (5)       |\n",
      "+-----------------+--------------------------------------------------------------------------+\n",
      "=================================\n",
      "\n",
      "\n",
      "Top 5 Overall Features\n",
      "=========================\n",
      "people (44), america (41), government (29), constitution (28), shall (27)\n",
      "=========================\n",
      "\n",
      "\n",
      "(k). Most Similar Texts\n",
      "=======================\n",
      "+-----------------+---------------------+\n",
      "| Text            | Most Similar Text   |\n",
      "+=================+=====================+\n",
      "| Biden_2021      | Trump_2017          |\n",
      "+-----------------+---------------------+\n",
      "| Kennedy_1961    | Biden_2021          |\n",
      "+-----------------+---------------------+\n",
      "| Lincoln_1861    | Washington_1789     |\n",
      "+-----------------+---------------------+\n",
      "| Trump_2017      | Biden_2021          |\n",
      "+-----------------+---------------------+\n",
      "| Washington_1789 | Lincoln_1861        |\n",
      "+-----------------+---------------------+\n",
      "=======================\n",
      "\n",
      "\n",
      "(k). Least Similar Texts\n",
      "========================\n",
      "+-----------------+----------------------+\n",
      "| Text            | Least Similar Text   |\n",
      "+=================+======================+\n",
      "| Biden_2021      | Washington_1789      |\n",
      "+-----------------+----------------------+\n",
      "| Kennedy_1961    | Lincoln_1861         |\n",
      "+-----------------+----------------------+\n",
      "| Lincoln_1861    | Kennedy_1961         |\n",
      "+-----------------+----------------------+\n",
      "| Trump_2017      | Lincoln_1861         |\n",
      "+-----------------+----------------------+\n",
      "| Washington_1789 | Biden_2021           |\n",
      "+-----------------+----------------------+\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people</th>\n",
       "      <th>america</th>\n",
       "      <th>government</th>\n",
       "      <th>constitution</th>\n",
       "      <th>shall</th>\n",
       "      <th>nation</th>\n",
       "      <th>states</th>\n",
       "      <th>let</th>\n",
       "      <th>country</th>\n",
       "      <th>union</th>\n",
       "      <th>...</th>\n",
       "      <th>crimes</th>\n",
       "      <th>culture</th>\n",
       "      <th>lower</th>\n",
       "      <th>crises</th>\n",
       "      <th>lot</th>\n",
       "      <th>crucial</th>\n",
       "      <th>loss</th>\n",
       "      <th>crucible</th>\n",
       "      <th>cultural</th>\n",
       "      <th>invisible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Biden_2021</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kennedy_1961</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lincoln_1861</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump_2017</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington_1789</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 2023 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 people  america  government  constitution  shall  nation  \\\n",
       "Biden_2021            9       20           0             3      2      14   \n",
       "Kennedy_1961          1        2           0             0      5       2   \n",
       "Lincoln_1861         20        0          18            24     17       0   \n",
       "Trump_2017           10       19           3             0      0       8   \n",
       "Washington_1789       4        0           8             1      3       2   \n",
       "Total                44       41          29            28     27      26   \n",
       "\n",
       "                 states  let  country  union  ...  crimes  culture  lower  \\\n",
       "Biden_2021            1    6        4      2  ...       0        1      1   \n",
       "Kennedy_1961          2   16        4      0  ...       0        0      0   \n",
       "Lincoln_1861         19    0        3     20  ...       1        0      0   \n",
       "Trump_2017            2    3        9      0  ...       0        0      0   \n",
       "Washington_1789       2    0        5      2  ...       0        0      0   \n",
       "Total                26   25       25     24  ...       1        1      1   \n",
       "\n",
       "                 crises  lot  crucial  loss  crucible  cultural  invisible  \n",
       "Biden_2021            1    1        0     0         1         0          0  \n",
       "Kennedy_1961          0    0        0     0         0         1          0  \n",
       "Lincoln_1861          0    0        0     1         0         0          0  \n",
       "Trump_2017            0    0        1     0         0         0          0  \n",
       "Washington_1789       0    0        0     0         0         0          1  \n",
       "Total                 1    1        1     1         1         1          1  \n",
       "\n",
       "[6 rows x 2023 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from functools import lru_cache\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import syllapy\n",
    "import tabulate\n",
    "from TextAnalyzer import TextAnalyzer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "folder_path = 'NLP_FIles/'\n",
    "\n",
    "# Read texts from the folder and initialize the analyzer\n",
    "texts = TextAnalyzer.read_texts_from_folder(folder_path)\n",
    "analyzer = TextAnalyzer(texts)\n",
    "\n",
    "# (a) Tokenization with uni-grams, without removing stopwords\n",
    "analyzer.tokenize(remove_stopwords=False)\n",
    "analyzer.display_token_counts(header=\"(a). Token Counts\")\n",
    "\n",
    "# (b) Tokenization with uni-grams, removing stopwords\n",
    "analyzer.tokenize()\n",
    "analyzer.display_token_counts(header=\"(b). Token Counts with no Stopwords\")\n",
    "\n",
    "# (c)~(f) Count specific tokens\n",
    "tokens_to_count = [\"america\", \"union\", \"freedom\", \"constitution\"]\n",
    "analyzer.display_specific_token_counts(tokens_to_count, header=\"(c)~(f). Specific Token Counts\")\n",
    "\n",
    "# (g) Tokenization with bi-grams\n",
    "analyzer.tokenize(ngram_range=(2, 2), dfm_key='bigrams')\n",
    "united_states_counts = analyzer.dfms['bigrams'].get('united states', pd.Series(0, index=analyzer.texts.keys()))\n",
    "data = list(zip(analyzer.texts.keys(), united_states_counts))\n",
    "analyzer.display_data(data, [\"Text\", \"united states\"], \"(g). Bi-gram Counts for 'united states'\")\n",
    "\n",
    "# (h) Analyze Token Counts to infer the theme of each text\n",
    "def print_theme_analysis():\n",
    "    divider = \"=\" * 50  # Extended for longer lines\n",
    "\n",
    "    print(\"\\n(h). Themes of Each Text\")\n",
    "    print(divider)\n",
    "\n",
    "    # Biden & Trump\n",
    "    print(\"\\nBiden & Trump:\")\n",
    "    print(\"- Tokens of Interest: 'America'\")\n",
    "    print(\"- Inference: Both use 'America' frequently, suggesting themes related to U.S.\\n  internal politics and the nation's future outlook. Their focus seems centered on\\n  evolving visions and potential trajectories for the United States.\")\n",
    "    print(divider)\n",
    "\n",
    "    # Lincoln\n",
    "    print(\"\\nLincoln:\")\n",
    "    print(\"- Tokens of Interest: 'Union' and 'Constitution'\")\n",
    "    print(\"- Inference: Lincoln's emphasis on 'Union' and 'Constitution' implies themes\\n  around the American Civil War. The national mood was heavily influenced\\n  by constitutional and legal matters, with tensions from potential southern\\n  states' secession and the overarching theme of national unity.\")\n",
    "    print(divider)\n",
    "\n",
    "    # JFK's Inaugural Address\n",
    "    print(\"\\nJFK's Inaugural Address:\")\n",
    "    print(\"- Tokens of Interest: 'freedom'\")\n",
    "    print(\"- Inference: JFK's emphasis on 'freedom' suggests a theme within the Cold War era,\\n  portraying it as a dichotomy between freedom and communism,\\n  emphasizing the pivotal role of freedom globally.\")\n",
    "    print(divider)\n",
    "\n",
    "    # Washington\n",
    "    print(\"\\nWashington:\")\n",
    "    print(\"- Tokens of Interest: 'union' and 'constitution'\")\n",
    "    print(\"- Inference: Washington's focus on 'union' and 'constitution' hints at themes\\n  of nation-building. Given the country's infancy and the ongoing constitution\\n  ratification, there was an emphasis on fostering unity and establishing\\n  a robust constitutional groundwork.\")\n",
    "    print(divider + \"\\n\")\n",
    "\n",
    "# Call the function\n",
    "print_theme_analysis()\n",
    "\n",
    "# (i) Compute FOG index\n",
    "fog_indexes = analyzer.compute_fog_indexes()\n",
    "data = [(text, \"{:.2f}\".format(fog)) for text, fog in fog_indexes.items()]\n",
    "analyzer.display_data(data, [\"Text\", \"FOG Index\"], \"(i). FOG Index for Each Text\")\n",
    "spacy_fog_indexes = analyzer.compute_fog_indexes(use_spacy=True)\n",
    "data = [(text, \"{:.2f}\".format(fog)) for text, fog in spacy_fog_indexes.items()]\n",
    "analyzer.display_data(data, [\"Text\", \"FOG Index\"], \"(i). FOG Index for Each Text (spaCy)\")\n",
    "\n",
    "# (j) Top 5 features for each text and display the DFM\n",
    "analyzer.top_features()\n",
    "analyzer.display_overall_top_features()\n",
    "\n",
    "# (k) Cosine similarity analysis\n",
    "most_similar_texts, least_similar_texts = analyzer.cosine_similarity_analysis()\n",
    "data_most_similar = [(text, most_similar_texts[text]) for text in most_similar_texts]\n",
    "data_least_similar = [(text, least_similar_texts[text]) for text in least_similar_texts]\n",
    "analyzer.display_data(data_most_similar, [\"Text\", \"Most Similar Text\"], \"(k). Most Similar Texts\")\n",
    "analyzer.display_data(data_least_similar, [\"Text\", \"Least Similar Text\"], \"(k). Least Similar Texts\")\n",
    "\n",
    "# Proof via Frequency Matrix\n",
    "freq_matrix = analyzer.generate_freq_matrix(dfm_key='base')\n",
    "column_sums = freq_matrix.sum(axis=0)\n",
    "sorted_columns = column_sums.sort_values(ascending=False).index\n",
    "freq_matrix_sorted = freq_matrix[sorted_columns]\n",
    "\n",
    "freq_matrix_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinancialAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
